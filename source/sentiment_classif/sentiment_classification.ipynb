{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "688b582e",
      "metadata": {
        "id": "688b582e",
        "outputId": "1a5c78b1-99cc-447a-8a33-addc70118ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "# Check if we have access to gpu\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the proper packages\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "gvm7joTpFneq",
        "outputId": "db41f899-a942-48a2-aaf5-88325d9c5ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gvm7joTpFneq",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Download data\n",
        "import requests\n",
        "request = requests.get(\"https://drive.google.com/uc?export=download&id=1wHt8PsMLsfX5yNSqrt2fSTcb8LEiclcf\")\n",
        "with open(\"data.zip\", \"wb\") as file:\n",
        "    file.write(request.content)\n",
        "\n",
        "# Unzip data\n",
        "import zipfile\n",
        "with zipfile.ZipFile('data.zip') as zip:\n",
        "    zip.extractall('data')"
      ],
      "metadata": {
        "id": "glDFnxl5tAaO"
      },
      "id": "glDFnxl5tAaO",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and set labels\n",
        "data_complaint = pd.read_csv('data/complaint1700.csv')\n",
        "data_complaint['label'] = 0\n",
        "data_non_complaint = pd.read_csv('data/noncomplaint1700.csv')\n",
        "data_non_complaint['label'] = 1\n",
        "\n",
        "# Concatenate complaining and non-complaining data\n",
        "data = pd.concat([data_complaint, data_non_complaint], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Drop 'airline' column\n",
        "data.drop(['airline'], inplace=True, axis=1)\n",
        "\n",
        "# Display 5 random samples\n",
        "data.sample(5)"
      ],
      "metadata": {
        "id": "nFYgGj7Ot9sH",
        "outputId": "52868bac-e061-4279-93e6-0f0593e61a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "id": "nFYgGj7Ot9sH",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                              tweet  label\n",
              "876    75687  @chrisodonnell @AlaskaAir That's not good. I h...      0\n",
              "737    23048  @united: Things to hate about flying United Ec...      0\n",
              "1819   13573  I side w/ @United &amp; @Uber on the ride-book...      1\n",
              "3362  167128  @josinei78 @AmericanAir @FrequentMiler I don't...      1\n",
              "2714   96537  Why flying stinks, and youÃ¢â‚¬â„¢re still pay...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b50de65-021b-487e-b145-69796d032a55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>75687</td>\n",
              "      <td>@chrisodonnell @AlaskaAir That's not good. I h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>23048</td>\n",
              "      <td>@united: Things to hate about flying United Ec...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1819</th>\n",
              "      <td>13573</td>\n",
              "      <td>I side w/ @United &amp;amp; @Uber on the ride-book...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3362</th>\n",
              "      <td>167128</td>\n",
              "      <td>@josinei78 @AmericanAir @FrequentMiler I don't...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2714</th>\n",
              "      <td>96537</td>\n",
              "      <td>Why flying stinks, and youÃ¢â‚¬â„¢re still pay...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b50de65-021b-487e-b145-69796d032a55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b50de65-021b-487e-b145-69796d032a55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b50de65-021b-487e-b145-69796d032a55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's randomly split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.tweet.values\n",
        "y = data.label.values\n",
        "\n",
        "X_train, X_val, y_train, y_val =\\\n",
        "    train_test_split(X, y, test_size=0.1, random_state=2022)"
      ],
      "metadata": {
        "id": "vsqvWMJgvb55"
      },
      "id": "vsqvWMJgvb55",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "66LUHMOmuj1a"
      },
      "id": "66LUHMOmuj1a",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('data/test_data.csv')\n",
        "\n",
        "# Keep important columns\n",
        "test_data = test_data[['id', 'tweet']]\n",
        "\n",
        "# Display 5 samples from the test data\n",
        "test_data.sample(5)"
      ],
      "metadata": {
        "id": "eJaORVVdnxqF",
        "outputId": "247ac9d4-fcb1-4278-a900-260200e0011a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "id": "eJaORVVdnxqF",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                              tweet\n",
              "1259   49419  Delivery of @AmericanAir 787s has now been del...\n",
              "4514  172040  Do something about the discrimination @united ...\n",
              "3633  137205  Hey @JetBlue how about updating your eta if th...\n",
              "1378   53980  @AmericanAir what's up with aa272 12/19? Any c...\n",
              "376    14917  I fly every single week, yet @united is the on..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16db02ed-2244-4897-9a63-78cb7cfd716d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1259</th>\n",
              "      <td>49419</td>\n",
              "      <td>Delivery of @AmericanAir 787s has now been del...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4514</th>\n",
              "      <td>172040</td>\n",
              "      <td>Do something about the discrimination @united ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3633</th>\n",
              "      <td>137205</td>\n",
              "      <td>Hey @JetBlue how about updating your eta if th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1378</th>\n",
              "      <td>53980</td>\n",
              "      <td>@AmericanAir what's up with aa272 12/19? Any c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>14917</td>\n",
              "      <td>I fly every single week, yet @united is the on...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16db02ed-2244-4897-9a63-78cb7cfd716d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16db02ed-2244-4897-9a63-78cb7cfd716d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16db02ed-2244-4897-9a63-78cb7cfd716d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "9Ji6Qwo7u4De"
      },
      "id": "9Ji6Qwo7u4De",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, Data2VecTextModel\n",
        "import torch\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"facebook/data2vec-text-base\")"
      ],
      "metadata": {
        "id": "FgL2IytMC9UU"
      },
      "id": "FgL2IytMC9UU",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum lenght of our sequencces\n",
        "all_tweets = np.concatenate([data.tweet.values, test_data.tweet.values])\n",
        "\n",
        "# Encode our concatenated data\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "# Find the maximum length\n",
        "max_len = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', max_len)"
      ],
      "metadata": {
        "id": "YdMvqAt95jhV",
        "outputId": "9afdb8f8-c058-4b3f-896d-5a55996d41d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YdMvqAt95jhV",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length:  167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 175"
      ],
      "metadata": {
        "id": "fs8e0yEF52wl"
      },
      "id": "fs8e0yEF52wl",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "iCFvv3Qh2jiq"
      },
      "id": "iCFvv3Qh2jiq",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "metadata": {
        "id": "9hfPrtHy26OH",
        "outputId": "9625b63e-59d6-43ac-9f50-c7315bc2ae82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9hfPrtHy26OH",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  @united I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on &amp; check in. Can you help?\n",
            "Token IDs:  [0, 100, 437, 519, 743, 4, 15267, 38, 769, 6298, 196, 13, 706, 722, 71, 38, 21, 3518, 7, 3598, 6, 122, 38, 64, 75, 7425, 15, 359, 1649, 11, 4, 2615, 47, 244, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Tokenizing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "b2X31FiC6ahJ"
      },
      "id": "b2X31FiC6ahJ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "lD1t25UZ61TH",
        "outputId": "216867fb-99c2-4199-c58b-e6ffd02cfba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lD1t25UZ61TH",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.99 ms, sys: 0 ns, total: 5.99 ms\n",
            "Wall time: 6.05 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class Data2VecClassifier(nn.Module):\n",
        "    \"\"\"Data2Vec Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(Data2VecClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.model = Data2VecTextModel.from_pretrained(\"facebook/data2vec-text-base\")\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.model(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "AeGnXA5s7JXp",
        "outputId": "16cbf0df-b9db-4c7e-98be-d20738289350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AeGnXA5s7JXp",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 36 µs, sys: 7 µs, total: 43 µs\n",
            "Wall time: 46 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    data2vec_classif = Data2VecClassifier(freeze=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    data2vec_classif.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(data2vec_classif.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return data2vec_classif, optimizer, scheduler"
      ],
      "metadata": {
        "id": "XP6dDUTv78vw"
      },
      "id": "XP6dDUTv78vw",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "iSBRfZGl8k1G"
      },
      "id": "iSBRfZGl8k1G",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "data2vec_classif, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(data2vec_classif, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "mP_cwVMg8yHc",
        "outputId": "511ac9ee-0da8-4b6d-b3e6-20f55465a2dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mP_cwVMg8yHc",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.weight', 'data2vec_text.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.694325   |     -      |     -     |   17.73  \n",
            "   1    |   40    |   0.698272   |     -      |     -     |   17.05  \n",
            "   1    |   60    |   0.693742   |     -      |     -     |   17.38  \n",
            "   1    |   80    |   0.692160   |     -      |     -     |   17.66  \n",
            "   1    |   95    |   0.690192   |     -      |     -     |   13.22  \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.693929   |  0.679418  |   66.53   |   86.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.650313   |     -      |     -     |   19.45  \n",
            "   2    |   40    |   0.593911   |     -      |     -     |   18.32  \n",
            "   2    |   60    |   0.601190   |     -      |     -     |   18.07  \n",
            "   2    |   80    |   0.603693   |     -      |     -     |   18.08  \n",
            "   2    |   95    |   0.592546   |     -      |     -     |   13.36  \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.609590   |  0.587866  |   71.59   |   90.65  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "dfD-fGmq82u7"
      },
      "id": "dfD-fGmq82u7",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A-cGqnoB_EX6"
      },
      "id": "A-cGqnoB_EX6",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = predict(data2vec_classif, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "metadata": {
        "id": "zKq_abXz-nuk",
        "outputId": "46c77b48-7858-4f5d-c84c-31555f327b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "id": "zKq_abXz-nuk",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.7637\n",
            "Accuracy: 71.47%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the train set and the validation set\n",
        "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
        "full_train_sampler = RandomSampler(full_train_data)\n",
        "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
        "\n",
        "# Train the Bert Classifier on the entire training data\n",
        "set_seed(42)\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(data2vec_classif, full_train_dataloader, epochs=2)"
      ],
      "metadata": {
        "id": "mdZ7Ob4r-tIw",
        "outputId": "4c73a47b-62aa-4dc1-eb23-e311a88a762a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mdZ7Ob4r-tIw",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.weight', 'data2vec_text.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.552215   |     -      |     -     |   18.31  \n",
            "   1    |   40    |   0.575907   |     -      |     -     |   17.96  \n",
            "   1    |   60    |   0.560806   |     -      |     -     |   17.91  \n",
            "   1    |   80    |   0.560716   |     -      |     -     |   17.48  \n",
            "   1    |   100   |   0.538895   |     -      |     -     |   17.37  \n",
            "   1    |   106   |   0.545394   |     -      |     -     |   4.51   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.556957   |     -      |     -     |   18.62  \n",
            "   2    |   40    |   0.549317   |     -      |     -     |   17.64  \n",
            "   2    |   60    |   0.552941   |     -      |     -     |   17.53  \n",
            "   2    |   80    |   0.546625   |     -      |     -     |   17.53  \n",
            "   2    |   100   |   0.563989   |     -      |     -     |   17.47  \n",
            "   2    |   106   |   0.556525   |     -      |     -     |   4.53   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(test_data.tweet)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ],
      "metadata": {
        "id": "M46ng7xP_SiW",
        "outputId": "8c7cecbd-d88f-457b-c037-5a9da810a450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "M46ng7xP_SiW",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = predict(data2vec_classif, test_dataloader)\n"
      ],
      "metadata": {
        "id": "vH0xsE1L_eZO",
        "outputId": "f758224f-a384-4e5a-fd5a-614c8e3965d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vH0xsE1L_eZO",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tweets predicted non-negative:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the probabilities\n",
        "threshold = 0.7\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"Number of tweets predicted non-negative: \", preds.sum())"
      ],
      "metadata": {
        "id": "bKJcTupUCT14",
        "outputId": "9e60477b-cf09-427c-fdba-419a49728d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bKJcTupUCT14",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tweets predicted non-negative:  943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = test_data[preds==1]\n",
        "list(output.sample(20).tweet)"
      ],
      "metadata": {
        "id": "kyMbiTBs_keT",
        "outputId": "00546552-5809-48ee-da8e-8e1b63132b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kyMbiTBs_keT",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@UtdArif @AmericanAir this is so shit',\n",
              " \"SFO for a weekend getaway in just a few short weeks! RT @AlaskaAir Can't wait to fly to ___________. http://t.co/oAhqQLPi0b\",\n",
              " '@MrAdamAp @AmericanAir boooooooooo which airport are you stuck in, btw?',\n",
              " 'HUGE thank you to @AlaskaAir for taking care of my wife today! Great #CustomerService when American Airlines failed. #jdpowersaward',\n",
              " '@SouthwestAir tring to change internationa itinareray usa to jamaica. All I get is music. is there a better number. # terrible',\n",
              " \"@americanair I can check in for tomorrow's flight that is cancelled?  How do I find out about rebooking info?\",\n",
              " \"@Julius_Thomas @AmericanAir that's too bad!  they should give you a couple of complimentary drinks next time!\",\n",
              " '@united literally you fuck everything over.',\n",
              " 'I really HATE @AmericanAir for always holding me a hostage to paid seat! #extortion #travel #American way',\n",
              " 'Darn! Im not targeted for the @HiltonHHonors promo for @AmericanAir Platinum  _ still looking to switch to AAdvantage from DL and AS.',\n",
              " 'You really suck @AmericanAir ',\n",
              " \"Hi @JetBlue. Are you expecting any delays from LGB to JFK tomorrow? I saw today's flts are delayed.\",\n",
              " '@Cowboycerrone @ColdConcept Lets hope like hell your not flying @AmericanAir this time, fuck those guys',\n",
              " \"@KnockoutOC @united what happened? I've never had a problem.\",\n",
              " \"@JoshMcDermitt @VirginAmerica HEYYYY not cool!!!! Don't you know who u he is???!!!!! Lol #GetJoshesBagHome USE THIS HASHTAG! !!! lol\",\n",
              " '@KayCockerill @AmericanAir @sfo Brutal.',\n",
              " \"So far I've found that as a person traveling with #disabilities @SouthwestAir is by far the best. @united is by far the WORST. #MyOpnion\",\n",
              " 'Shame on you @United! http://t.co/2g5rpywdPc',\n",
              " '@dsqueezy22 @AmericanAir are you upset?',\n",
              " '.@AmericanAir featured @The_Alexander in their September issue! Check it out &gt;&gt; http://t.co/tEcjc1oJXv']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "egDlTVusBmt_"
      },
      "id": "egDlTVusBmt_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "sentiment_classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}